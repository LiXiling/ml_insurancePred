{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Insurance Claim Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "This section loads the dataset and prints the first 5 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
      "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
      "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
      "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
      "3  10    B    B    A    B    A    A    A    A    B   ...     0.440945   \n",
      "4  11    A    B    A    B    A    A    A    A    B   ...     0.178193   \n",
      "\n",
      "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
      "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
      "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
      "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
      "3  0.391128  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077   \n",
      "4  0.247408  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011   \n",
      "\n",
      "     cont14     loss  \n",
      "0  0.714843  2213.18  \n",
      "1  0.304496  1283.60  \n",
      "2  0.774425  3005.09  \n",
      "3  0.602642   939.85  \n",
      "4  0.432606  2763.85  \n",
      "\n",
      "[5 rows x 132 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "visualization = False\n",
    "\n",
    "# Load the provided datasets\n",
    "data = pd.read_csv('data/train.csv')\n",
    "\n",
    "# Drop 'loss' and 'id' from the feature set and save target feature 'loss' in its own variable\n",
    "claims = data['loss']\n",
    "features = data.drop(['id','loss'], axis = 1)\n",
    "\n",
    "print data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Explore the Data\n",
    "To get a better understanding of the provided data, some analysis and visualization is provided.\n",
    "### Statistical Analysis\n",
    "This section shall detail some of the statistical properties of the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def statistics(p_data):\n",
    "    #Minimum claim of the data\n",
    "    minimum = np.amin(p_data)\n",
    "\n",
    "    #Maximum claim of the data\n",
    "    maximum = np.amax(p_data)\n",
    "\n",
    "    #Mean claim of the data\n",
    "    mean = np.mean(p_data)\n",
    "\n",
    "    #Median price of the data\n",
    "    median = np.median(p_data)\n",
    "\n",
    "    #25% Quantile\n",
    "    percentile_25 = np.percentile(p_data, 25)\n",
    "\n",
    "    #50% Percentile\n",
    "    percentile_50 = np.percentile(p_data, 50)\n",
    "\n",
    "    #75% Quantile\n",
    "    percentile_75 = np.percentile(p_data, 75)\n",
    "\n",
    "    #Standard deviation of the claims in the data\n",
    "    std_claim = np.std(p_data)\n",
    "\n",
    "    # Show the calculated statistics\n",
    "    print \"Statistics for Insurence claims dataset:\\n\"\n",
    "    print \"Minimum claim: ${:,.2f}\".format(minimum)\n",
    "    print \"Maximum claim: ${:,.2f}\".format(maximum)\n",
    "    print \"Mean claim: ${:,.2f}\".format(mean)\n",
    "    print \"Median claim ${:,.2f}\".format(median)\n",
    "    print \"25% percentile ${:,.2f}\".format(percentile_25)\n",
    "    print \"50% percentile ${:,.2f}\".format(percentile_50)\n",
    "    print \"75% percentile ${:,.2f}\".format(percentile_75)\n",
    "    print \"Standard deviation of claims: ${:,.2f}\".format(std_claim)\n",
    "    \n",
    "if visualization:\n",
    "    statistics(claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This information helps to give context to future predictions. Also the Minimum of \\$0.67 and Maximum of \\$121,012.25, are already showing that some outliers will have to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scatterplot matrix helps to get a better understanding of the data.\n",
    "For performance reasons, the data is split to only contain the first 500 points. **It might still take a while to compute!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if visualization:    \n",
    "    scatterdata = data[:500].drop(\"id\", axis = 1)\n",
    "    pd.scatter_matrix(scatterdata, alpha = 0.3, figsize = (14,8), diagonal = 'kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can already see correlations, that can be used to trim down the dataset using feature selection. Most notably are cont11 <-> cont12, which seem to correlate strictly linearly. Some other correlations can also be seen, but not as clearly, like cont1 <-> cont9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the scatterplot matrix over **ALL** the datapoints:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/img/scatterplot_all.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It can be produced by the following code:\n",
    "\n",
    "**THIS WILL TAKE QUITE A LONG TIME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Activate if wished. Again: This will be slow\n",
    "if False:\n",
    "    pd.scatter_matrix(data.drop('id', axis = 1), alpha = 0.3, figsize = (14,8), diagonal = 'kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatterplot matrix also shows the right skewedness of the target feature 'loss':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if visualization:\n",
    "    claims.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target feature needs to be normalized for the regression model to yield the best results possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "claims_normalized = np.log(claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalized data can then be visualized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if visualization:\n",
    "    claims_normalized.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical features need to be label encoded and then one hot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection\n",
    "\n",
    "The big difference between Maximum and Minimum, and their distance to the nearest quartile already show some outliers need to be removed.\n",
    "\n",
    "The following code will remove all the data points with a bigger distance to the nearest quartile than 1.5 * the interquartile range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: 7.0937866127\n",
      "Q3: 8.25946984122\n",
      "Step: 1.74852484278\n",
      "\n",
      "Example of outliers:\n",
      "89       3.648057\n",
      "470      5.260408\n",
      "713     10.164725\n",
      "867     10.295591\n",
      "1015    10.124239\n",
      "Name: loss, dtype: float64\n",
      "\n",
      "Original data had 188318 rows. \n",
      "521 outliers were removed and good_data now holds 187797 rows\n"
     ]
    }
   ],
   "source": [
    "#USE NORMALIZED AND QUANTILE STEP!!!\n",
    "q1 = np.percentile(claims_normalized, 25)\n",
    "q3 = np.percentile(claims_normalized, 75)\n",
    "\n",
    "\n",
    "# Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n",
    "step = 1.5 * (q3 - q1)\n",
    "\n",
    "# Print of Q1 and Q3 and Step to better understand the outliers:\n",
    "print \"Q1:\", q1\n",
    "print \"Q3:\", q3\n",
    "print \"Step:\", step\n",
    "\n",
    "log_data = pd.concat([features, claims_normalized], axis = 1)\n",
    "outliers = log_data[~((log_data['loss'] >= q1 - step) & (log_data['loss'] <= q3 + step))]\n",
    "\n",
    "print \"\\nExample of outliers:\"\n",
    "print outliers.head()['loss']\n",
    "\n",
    "# Remove the outliers. Unfortunately had problems with Dataframe Subtraction, so negated the query above\n",
    "good_data = log_data[((log_data['loss'] >= q1 - step) & (log_data['loss'] <= q3 + step))]\n",
    "\n",
    "# Save num of Records. OneHotEncoder adds several values with NaN leading to errors\n",
    "numOfRecords = good_data.shape[0]\n",
    "\n",
    "print \"\\nOriginal data had {:} rows. \\n{:} outliers were removed and good_data now holds {:} rows\".format(\n",
    "    log_data.shape[0], outliers.shape[0], numOfRecords\n",
    ")\n",
    "\n",
    "claims_clean = good_data['loss'].reset_index(drop = True)\n",
    "features_clean = good_data.drop('loss', axis = 1).reset_index(drop = True)\n",
    "\n",
    "# Free memory\n",
    "del outliers\n",
    "del log_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cat1  cat2  cat3\n",
      "0     0     1     0\n",
      "1     0     1     0\n",
      "2     0     1     0\n",
      "3     1     1     0\n",
      "4     0     1     0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Label Encode all the categorical features\n",
    "for i in range(1,130):\n",
    "    key = 'cat' + str(i)\n",
    "    #Failsafe IF-Clause for column adressing\n",
    "    if key in features_clean.columns:\n",
    "        features_clean[key] = le.fit_transform(features_clean[key])\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "print features_clean.head()[['cat1','cat2','cat3']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onehot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make Encoder\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "\n",
    "#Split Categorical Columns\n",
    "#print features_clean.iloc[:,0:116].head()\n",
    "catF = features_clean.iloc[:,0:116]\n",
    "#print catF.shape\n",
    "#Fit Categorigal Columns\n",
    "enc.fit(catF)\n",
    "\n",
    "#Encode Categorical Columns\n",
    "catEnc = enc.transform(catF).toarray()\n",
    "catEnc = pd.DataFrame(catEnc)\n",
    "#print catEnc.shape\n",
    "#catEnc2 = pd.concat([catEnc, pd.DataFrame(enc.transform(catF.iloc[90001:]).toarray())], axis = 0)\n",
    "#catEnc2 = pd.DataFrame(enc.transform(catF.iloc[90001:]).toarray())\n",
    "#print catEnc.head()\n",
    "#print catEnc.tail()\n",
    "\n",
    "# Concatinate Categorical and Continous Columns\n",
    "features_encoded = pd.concat([catEnc, features_clean.iloc[:,116:]], axis=1, join_axes=[catEnc.index])\n",
    "#features_encoded = pd.concat([catEnc, features_clean.iloc[:,116:]], axis = 1)#[:numOfRecords]\n",
    "#print features_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply PCA by fitting the reduced data\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50).fit(features_encoded)\n",
    "reduced_data = pca.transform(features_encoded)\n",
    "\n",
    "explainedVar = pca.explained_variance_ratio_\n",
    "print np.sum(explainedVar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#Simple Switch to influence testing set size for performance\n",
    "if True:\n",
    "    size = 20000\n",
    "else:\n",
    "    size = None\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    reduced_data[:size], claims_clean[:size], test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scorer Function\n",
    "\n",
    "As I work with normalized log(data), define a custome scoring function with integrated np.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def scorerFunc(test, pred):\n",
    "    return mean_absolute_error(np.exp(test), np.exp(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "\n",
    "This is the benchmark, the models will compete against.\n",
    "The benchmark model will always predict the mean of the training set. The performance is measured with the mean absolute error as requested by the Kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The benchmark performance is 1796.33100449\n"
     ]
    }
   ],
   "source": [
    "benchMAE = scorerFunc(y_test, np.full_like(y_test, np.mean(y_train)))\n",
    "print \"The benchmark performance is {:}\".format(benchMAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104.06524998\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeRegressor()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print scorerFunc(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1489.60699214\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVR()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print scorerFunc(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NOTES"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "smallifiedFeatures = features\n",
    "for i in range(1, 78):\n",
    "    smallifiedFeatures = smallifiedFeatures.drop('cat' + str(i), axis=1)\n",
    "print smallifiedFeatures.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "enc = preprocessing.OneHotEncoder()\n",
    "\n",
    "trans = enc.fit_transform(features_clean[['cat1','cat116']])\n",
    "\n",
    "print features_clean.head()\n",
    "print trans\n",
    "print len(features_clean.columns)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#print features_encoded.head()\n",
    "#print features_encoded.tail()\n",
    "\n",
    "print catEnc.shape\n",
    "print features_clean.shape\n",
    "print features_encoded.shape\n",
    "print features_clean.iloc[:,:116].shape\n",
    "\n",
    "print features_clean.iloc[89]\n",
    "print features_encoded.iloc[85:91]\n",
    "# Check for NaN Values\n",
    "print features_encoded.isnull().sum().sum()\n",
    "#print features_encoded.isnull().sum()\n",
    "#df = features_encoded\n",
    "#print df[df.isnull().any(axis=1)]\n",
    "\n",
    "#print features_encoded.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
